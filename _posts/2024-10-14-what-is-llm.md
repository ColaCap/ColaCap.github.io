---
title: "🧠 LLM을 이해하기 위한 기본 개념들"
date: 2024-10-14
modified_date: 2024-10-14
---

# 🧠 LLM을 이해하기 위한 기본 개념들

이 가이드는 LLM, RAG, GPT, LLaMA, Claude, LangChain, KoboldAI, TavernAI, SillyTavern, ChatRTX 같은 알 수 없는 용어들이 인터넷에 남발하는데, 각각 무엇이고 무슨 차이가 있고 왜 존재하는지 이해하기 어려워서 지피티에게 이것 저것 물어보고 대화한 것들을 지피티 칸바스를 통해서 다 정리해달라고 한거야.
인공지능이 알려준 정보라 간략한 팩트체크를 하긴 했지만, 놓친 부분이 있을 수 있어.

## 1. 🤖 LLM 모델의 종류

- **💰 유료 모델**: GPT-4, Claude 같은 모델들은 ☁️ 클라우드에서 제공되고 사용할 때 비용이 들어. 주로 상업적인 목적으로 사용돼.
- **🆓 오픈소스 모델**: LLaMA, GPT-2, GPT-J, GPT-Neo 같은 모델들은 무료로 사용할 수 있고 네 🖥️ 컴퓨터에서 직접 실행할 수 있어. 오픈소스 모델은 자유롭게 수정하고 배포할 수 있다는 장점이 있어.

## 2. 🔍 RAG (Retrieval-Augmented Generation)

- **RAG**는 LLM이 외부 📄 문서나 데이터베이스에서 정보를 직접 읽지 못하기 때문에 필요한 기술이야. 외부 데이터를 검색해서 모델에 전달해, 더 정확하고 맥락에 맞는 답을 만들어낼 수 있게 도와줘.
- **🌐 인터넷 검색**도 RAG를 이용해서 가능해. 예를 들어, **LangChain** 같은 도구를 사용하면 인터넷에서 최신 정보를 찾아서 LLM에 제공할 수 있어.
- **LangChain**은 RAG를 구현하기 위해 외부 데이터베이스와 연결하고 검색 기능을 제공해서, 모델이 실시간으로 필요한 정보를 사용할 수 있게 해.

## 3. 🖥️ 로컬 LLM 구동에 필요한 기본 요소

- **🐍 Python**, **🔥 PyTorch**, **🚀 CUDA** 같은 프로그램이 필요해. 특히 **NVIDIA GPU**가 있다면 **CUDA**를 사용해 모델 성능을 높일 수 있어.
- **파이썬 코드**로 직접 모델을 실행하거나, **Oobabooga**, **KoboldAI**, **KoboldCpp** 같은 프로그램을 이용해 쉽게 모델을 실행할 수 있어. **KoboldCpp**는 특히 CPU에서 경량으로 LLM을 실행할 때 유용해.
- **TensorRT** 같은 도구를 사용하면 GPU 성능을 더 최적화할 수 있어.

## 4. 🖥️💬 프론트엔드와 백엔드

- **LLM과 대화**하려면 **TavernAI** 같은 **프론트엔드 프로그램**이 필요해. 이 프로그램은 사용자가 LLM과 대화할 수 있게 해주는 인터페이스 역할을 해.
- **Oobabooga** 같은 프로그램은 **백엔드와 프론트엔드**를 함께 제공해서, 모델을 실행하고 사용자 인터페이스를 관리하기 쉽게 만들어줘.
- **TavernAI**는 **프론트엔드**만 제공하니까, 사용하려면 **Oobabooga** 같은 **백엔드** 프로그램과 함께 써야 해.
- **SillyTavern**은 **TavernAI**의 확장판이야. 더 많은 설정과 다양한 백엔드를 지원해. **KoboldAI**, **Oobabooga**, **ChatRTX** 같은 여러 백엔드와 잘 맞아.

## 5. 🧠 LLM의 메모리

- **LLM 모델**은 기본적으로 대화의 맥락을 기억하는 **메모리** 기능이 없어. 외부 도구를 사용해서 이 기능을 추가할 수 있어.
- **🕒 숏텀 메모리 (Short-term Memory)**: 모델이 대화의 맥락을 기억하는 건 **토큰 제한 범위** 내에서만 가능해. **LangChain**이나 **Oobabooga** 같은 도구가 이 역할을 해서, 대화의 흐름을 유지할 수 있어.
- **🗄️ 롱텀 메모리 (Long-term Memory)**: 예전 대화를 영구적으로 기억하려면 **데이터베이스**가 필요해. 이 방식으로 모델이 몇 년 전의 대화도 기억할 수 있어. **Pinecone**, **Weaviate**, **MongoDB** 같은 데이터베이스를 사용해서 대화 내용을 저장하고 필요할 때 꺼내 쓸 수 있어.

## 6. 📊 효율적인 데이터 검색을 위한 인덱싱

- **RAG**가 외부 데이터를 효율적으로 검색하려면 **Elasticsearch**나 **FAISS** 같은 **인덱싱 프로그램**이 필요해.
- **🔍 Elasticsearch**는 키워드 검색에 뛰어나고, 텍스트와 구조화된 데이터를 잘 검색할 수 있어.
- **🔎 FAISS**는 벡터 유사도 검색에 적합해서, 개념적으로 비슷한 내용을 찾는 데 좋아.
- **Chroma** 같은 오픈소스 벡터 데이터베이스도 사용할 수 있어. 이건 RAG 시스템에서 빠르게 벡터 검색을 할 수 있게 해줘.

## 7. 🚀 ChatRTX

- **ChatRTX**는 NVIDIA RTX GPU를 이용해 LLM을 네 컴퓨터에서 실행할 수 있게 해주는 솔루션이야. **TensorRT-LLM**을 이용해 모델의 성능을 최적화하고, **RAG** 기능을 통해 네 데이터를 모델에 통합할 수 있어. 이러면 모델이 더 빠르고 효율적으로 작동하면서 너한테 맞는 답변을 줄 수 있어.
- **ChatRTX**는 모델 실행, **LangChain**을 통한 RAG, **인덱싱 도구**(예: **Elasticsearch**나 **FAISS**), **메모리 관리**(숏텀 및 롱텀 메모리) 등을 하나의 솔루션으로 제공해. **ChatRTX** 하나만 설치하면, LLM을 네 컴퓨터에서 실행하고 RAG와 메모리 기능을 모두 활용할 수 있어. 설정도 아주 간단해.

## 8. 🔒 데이터 프라이버시

- 로컬에서 LLM을 실행하는 가장 큰 장점 중 하나는 **데이터 프라이버시**야. ☁️ 클라우드 기반 모델과는 다르게, 모든 데이터 처리가 네 컴퓨터 안에서 이루어져. 그래서 너의 개인 정보나 대화 내용이 외부 서버로 보내지지 않아. 이 덕분에 개인 정보 보호가 잘 돼.

## 9. ⚙️ 툴 간 비교

| 툴 이름 | 역할 | 백엔드/프론트엔드 | 주요 특징 |
| --- | --- | --- | --- |
| **KoboldCpp** | 백엔드 | CPU 기반 | CPU에서 LLM을 경량으로 실행 가능 |
| **KoboldAI** | 백엔드 | GPU 기반 | 💬 대화와 스토리텔링에 최적화된 인터페이스 |
| **Oobabooga** | 백엔드/프론트엔드 | GPU 기반 | 백엔드와 프론트엔드를 모두 제공, 다용도로 사용 가능 |
| **TavernAI** | 프론트엔드 | - | 대화 인터페이스 제공, 별도의 백엔드 필요 |
| **SillyTavern** | 프론트엔드 | - | TavernAI의 확장판, 더 많은 백엔드 지원 |
| **ChatRTX** | 통합 솔루션 | GPU 기반 | RAG, 메모리, 모델 실행을 한 번에 통합 |

## 10. 🔗 프론트엔드와 백엔드의 상호작용

- **프론트엔드**랑 **백엔드**는 서로 소통하면서 LLM의 기능을 제공해. 예를 들어, **TavernAI** 같은 프론트엔드는 **Oobabooga**나 **KoboldAI** 같은 백엔드랑 **API**를 통해 연결돼서 데이터를 주고받아.
- **프론트엔드**는 사용자와의 상호작용을 담당하고, **백엔드**는 실제 모델을 실행해서 결과를 제공해. 사용자가 질문을 입력하면, 프론트엔드가 그걸 백엔드로 보내고, 백엔드가 답변을 만들어 다시 프론트엔드로 보내서 사용자에게 보여주는 식이야.
- 이런 **상호작용 구조**는 LLM의 유연성을 높여주고, 서로 다른 프론트엔드와 백엔드를 조합해서 다양한 사용자 경험을 제공할 수 있게 해.
